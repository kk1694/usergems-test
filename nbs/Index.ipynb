{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d22589",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba0cdd",
   "metadata": {},
   "source": [
    "## 1. Optional: create virtual env\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "\n",
    "venv/bin/activate\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1b321",
   "metadata": {},
   "source": [
    "## 2. Install Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9640638",
   "metadata": {},
   "source": [
    "```\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Note, this does not include the finetuning notebook dependencies (pytorch, unsloth, etc.).\n",
    "\n",
    "Note, if you want to re-run the data-processing (i.e. `/scripts` folder files), use an editable install instead (`pip intall -e .`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434b1d8",
   "metadata": {},
   "source": [
    "## 3. Add OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7ebe4",
   "metadata": {},
   "source": [
    "Add a `.env` file in the project root (or just set it outside)\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=XXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454a77b",
   "metadata": {},
   "source": [
    "## 4. Use the CLI\n",
    "\n",
    "The qualifier CLI is automatically installed:\n",
    "```\n",
    "ugfind https://notion.so\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48634c1",
   "metadata": {},
   "source": [
    "## 5. Use the qualified list for outreach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f21121",
   "metadata": {},
   "source": [
    "You find the csv (with an example copy) under `data/06_qualified.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ba811",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459564b3",
   "metadata": {},
   "source": [
    "## 1. Signals for Usergems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878a9ba",
   "metadata": {},
   "source": [
    "Here are some UG signal ideas:\n",
    "- From a list of signals, what would be the top 3 for a company, how well do they match/how relevant are they (the company description)?\n",
    "    - Hi X, ... at UG we could give you the following 3 signals (that you can't get anywhere else):\n",
    "        - S1\n",
    "        - S2\n",
    "        - S3\n",
    "- Match closest customer\n",
    "    - From a list of UG customers, who is the closest one? And then match up (and qualify if it's a good match)\n",
    "    - Hi X, we work with {company name}, who similar to you is doing X. They use UG for Y. Want to check it out?\n",
    "- Relying on jobs\n",
    "     - are they hiring?\n",
    "     - are they hiring AE/SDRs?\n",
    "     - new sales inititiatives?\n",
    "     - are they advertising in their jobs any of the tools you integrate with?\n",
    "- Do they have a product / service that needs a sales team (i.e. not self serve)\n",
    "    - can't use this in copy, but gets rid of \"university press\", gets rid of not launched products, gets rid of self-serve products\n",
    "     \n",
    "     \n",
    "From a business perspective, finetuning (a local model) is not worth it in this case.\n",
    "- 60k companies -- not that much cost savings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df7762",
   "metadata": {},
   "source": [
    "## After manually reviewing first 20 sites (CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707844b",
   "metadata": {},
   "source": [
    "The input data is quite noisy. Out of the first 20 companies (computer software)\n",
    "- 7 pages didn't even load\n",
    "- 2 are not CS companies\n",
    "\n",
    "After reviewing the website: another good signal is \"are they selling a product/service to end customers\"\n",
    "- gets rid of no loa\n",
    "- gets rid of \"Michigan Unversity Press\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc6577",
   "metadata": {},
   "source": [
    "## 2. Finetuning\n",
    "\n",
    "- Left too little time, and had no GPU... so had to set up server on AWS & SSH\n",
    "- Also only trained for ~10min (single GPU) for demonstration, real life would train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508bc0ad",
   "metadata": {},
   "source": [
    "## 3. Improvements/tests I didn't have time for\n",
    "- Use a DB instead of a sequence of csv files\n",
    "    - even just SQLite would be good\n",
    "- Optimize scraper\n",
    "    - Use stealth mode / anti-anti-scraping tech\n",
    "    - Investigate if there are major failures\n",
    "    - Add retries\n",
    "    - Add more elaborae JS waiting if needed\n",
    "    - Record redirects for deduplication\n",
    "- html2markdown\n",
    "    - I know there is a benchmark of different repos, but couldn't find it\n",
    "    - There are also a bunch of new repos I didn't see previously\n",
    "    - Do some side-by-side comparisons for which markdown converter is the best (vs. vanially html!)\n",
    "    - Do more html stripping (i.e. what I do for images)\n",
    "        - Good way to go about it is inspect the HTML length, markdown length, and the ratios\n",
    "    - Sometimes text whitespace gets too squashed. Need to inspect where this happens\n",
    "- Invalid websites\n",
    "    - ~50% of invalid websites are actually valid, so this part can be made better.\n",
    "        - some we couldn't scrape\n",
    "        - some got mistakenly classified\n",
    "    - it's ~10% of the list, so there is still juice here\n",
    "- Job classification\n",
    "    - open apply & no jobs gets frequently confused with edge cases. Product-wise it's the least important destinction, but for a proper system, need some more time on this\n",
    "- Job extraciton\n",
    "    - This really should be a separate step, but didn't have time so just hacked it onto the classification\n",
    "- General GPT calls\n",
    "    - Definitely need more evals\n",
    "    - With a good eval set can do some prompt exploration\n",
    "    - Should add production checks (i.e. if the job judgment is job list, the list should be non-empty)\n",
    "- Finetuning\n",
    "    - Super little time on this, so just doing very high-level\n",
    "    - Used Unsloth starter notebook\n",
    "    - To do properly:\n",
    "        - Don't do on completely separate notebook, integrate with rest of the codebase \n",
    "        - Check context sizes - might need to trim input or increase unsloth context\n",
    "        - Check different models & settings. \n",
    "        - Train on much larger dataset \n",
    "            - Also make sure the training dataset is actually clean\n",
    "        - Train for longer (until validation metrics deteriorate)\n",
    "        - Have separate valid & test set\n",
    "        - Train whole pipeline / schema, not just classification\n",
    "        - Set up instructor!\n",
    "            - Right now we're just hoping LLAMA gets it right\n",
    "        - And obviously proper deployment\n",
    "- Whole flow:\n",
    "    - Check what is happening with the loops; it's ridiculously high\n",
    "- Add a web app for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fa3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
